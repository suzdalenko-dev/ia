# üìö Calendario IA 12 Meses + Predicci√≥n de Compras ‚Äî **README.md**

> Plan paso a paso para convertirte en profesional de IA aplicando tu stack full‚Äëstack (**HTML/JS/PHP/Java/Python/Kotlin/Android/SQL/SQLite/Laravel/Symfony/Django/Vue**).  
> Resultado: **10+ proyectos** en portafolio, **APIs listas para producci√≥n** y un **motor de compras** (forecast ‚Üí cantidad a pedir) con m√©tricas de negocio.

---

## üß≠ C√≥mo usar este plan

- **Estructura por tema**: *Qu√© es ‚Üí Para qu√© sirve ‚Üí C√≥mo estudiarlo ‚Üí Pitfalls ‚Üí Entregables ‚Üí Recursos ‚Üí C√≥mo lo usar√°s en proyectos*.
- **Tiempo**: 6‚Äì10 h/semana. Cada semana tiene objetivos concretos.
- **Repositorio base** (recomendado):
  ```text
  project/
    data/            # raw/processed/interim/external
    notebooks/
    src/
    models/
    api/
    reports/
    docker/
    .env.example
    requirements.txt / pyproject.toml
    Makefile
  ```
- **Workflow**: (1) Leer, (2) Reproducir, (3) Re‚Äëimplementar sin mirar, (4) Aplicar a tus datos, (5) Documentar.

---

# A) √Ålgebra, C√°lculo y Probabilidad ‚Äúm√≠nimas‚Äù para IA (Semana 1.5)

> Inserta esta **Semana 1.5** entre tus Semanas 1 y 2. √ösala tambi√©n como **ficha de consulta** todo el a√±o.

## 1) √Ålgebra lineal esencial

**Qu√© es**  
- Espacios vectoriales y **normas**: \(\ell_1,\ \ell_2,\ \ell_\infty\); normalizaci√≥n/escalado.  
- **Producto escalar** y **proyecciones** (descomposici√≥n ortogonal). Proyecci√≥n: \(P = X(X^\top X)^{-1} X^\top\).  
- **Matrices**: rango, traza, determinante, inversa, sim√©tricas, **PSD** (\(x^\top A x \ge 0\)).  
- **Autovalores/autovectores**: estabilidad num√©rica, condicionamiento.  
- **SVD**: \(X = U\,\Sigma\,V^\top\) ‚Üí reducci√≥n de dimensi√≥n, compresi√≥n, **PCA**.  
- **PCA**: varianza explicada, *whitening*.

**Para qu√© sirve**  
- Entender la **geometr√≠a de los datos**, compresi√≥n/ruido, reducci√≥n de dimensi√≥n y **descorrelaci√≥n**.
- Fundamental para **recomendadores**, **visi√≥n** (bases ortogonales), **clustering** y **explicabilidad**.

**C√≥mo estudiarlo**  
- Implementa **PCA** con **SVD** desde cero y comp√°ralo con `sklearn`.  
- Juega con **escalado** (z‚Äëscore, min‚Äëmax) y mide el impacto en k‚Äëmeans/svm.

**Pitfalls**  
- Invertir matrices ‚Äúa pelo‚Äù. Preferir **Cholesky/QR/SVD**.  
- Aplicar PCA sin estandarizar *features* que est√°n en escalas distintas.

**Entregables**  
- **Cheat‚Äësheet** A4 con identidades (proyecci√≥n, traza, derivadas).  
- Notebook de **PCA por SVD** + *plot* de varianza explicada.

**Recursos r√°pidos**  
- ‚ÄúLinear Algebra Review‚Äù (CS229).  
- ‚ÄúMatrix Cookbook‚Äù (identidades y derivadas).

**C√≥mo lo usar√°s en proyectos**  
- **Mes 2‚Äì4**: PCA para visualizaci√≥n 2D de clientes (segmentaci√≥n).  
- **Mes 9 (RAG)**: reducci√≥n de dimensi√≥n para *debug* de **embeddings**.  
- **Mes 10 (visi√≥n)**: comprensi√≥n de **timm/Conv** y compresi√≥n con SVD/ONNX.

---

## 2) C√°lculo (vectorial y matricial)

**Qu√© es**  
- **Gradiente**, **Jacobiano**, **Hessiano**; **regla de la cadena**.  
- **C√°lculo matricial** √∫til:
  - \(\frac{\partial}{\partial X}\,\mathrm{tr}(AX) = A^\top\)  
  - \(\frac{\partial}{\partial X}\,\lVert AX-b\rVert_2^2 = 2A^\top(AX-b)\)  
  - \(\frac{\partial}{\partial X}\,\log\det X = (X^{-1})^\top\) (X sim√©trica PD).

**Para qu√© sirve**  
- Entender **backprop**, **regularizaci√≥n** y **optimizaci√≥n** de p√©rdidas (por qu√© funciona Adam, por qu√© *early stopping* regulariza).

**C√≥mo estudiarlo**  
- Deriva la **MSE** y la **log√≠stica**.  
- Calcula a mano un paso de **descenso de gradiente** en regresi√≥n.

**Entregables**  
- Notebook con derivadas y comparaci√≥n con *autograd* (PyTorch).

**Recurso claro**  
- ‚ÄúThe Matrix Calculus You Need for Deep Learning‚Äù.

**C√≥mo lo usar√°s en proyectos**  
- Ajustar **LR/weight decay** con criterio y diagnosticar **overfitting** vs **underfitting**.  
- Implementar p√©rdidas custom (ej. **WAPE/SMAPE** para **forecast**).

---

## 3) Optimizaci√≥n

**Qu√© es**  
- **Convexidad** (Jensen), **Lipschitz**, **condicionamiento**.  
- **GD/Momentum/Adam**; **early stopping** y **regularizaci√≥n** (L1/L2).  
- **Lagrangiano** y **multiplicadores** (intuici√≥n pr√°ctica).

**Para qu√© sirve**  
- Convergencia **m√°s estable** y modelos que **generalizan** mejor.  
- Dise√±ar *schedules* de LR y *weight decay* adecuados a tu problema.

**C√≥mo estudiarlo**  
- Comparativa de **SGD vs Adam** en el mismo MLP (curvas *loss/val*).

**Entregables**  
- Experimentos registrados en **MLflow**, con tabla de *runs* y m√©tricas.

**C√≥mo lo usar√°s en proyectos**  
- **Mes 4‚Äì5**: entrenos de **transfer learning** estables.  
- **Mes 6‚Äì7**: *tuning* de ETS/ARIMA con *grid/random* bien acotado.

---

## 4) Probabilidad y estad√≠stica

**Qu√© es**  
- Distribuciones: **Bernoulli, Binomial, Poisson, Exponencial, Normal**.  
- Momentos: \(\mathbb E[X]\), **Var/Cov**, matrices de covarianza.  
- **MLE/MAP**, intervalos de confianza, **bootstrap**.  
- **Entrop√≠a**, **cross‚Äëentropy**, **KL** (clasificaci√≥n y VAEs).  
- Series: **autocovarianza**, **ACF/PACF** (para **Mes 6**).

**Para qu√© sirve**  
- Elegir **m√©tricas** y **intervalos** realistas; entender **riesgo** y **incertidumbre** en predicci√≥n de demanda.

**Pitfalls**  
- Confundir varianza poblacional vs muestral.  
- Mezclar escalas/unidades sin normalizar.

**Entregables**  
- Notebook: estimaci√≥n de \(\mu\) y \(\sigma\) con **bootstrap** + bandas de confianza.

**C√≥mo lo usar√°s en proyectos**  
- **Mes 7**: stock de seguridad (\(SS\)) y punto de pedido (\(ROP\)) con **Z‚Äëscores**.  
- **Mes 3**: *threshold tuning* de clasificadores con curvas **PR** y coste.

---

## 5) Entregables r√°pidos (resumen)

- **Hoja de f√≥rmulas** (A4) con 25 identidades (traza, log‚Äëdet, proyecciones).  
- Notebook de ejercicios:
  1. Deriva **MSE** y **log√≠stica**.
  2. Implementa **PCA por SVD** y comp√°ralo con `sklearn`.
  3. Estima \(\sigma\) y \(\mu\) con **bootstrap** y grafica intervalos.

---

# B) Ecosistema de bibliotecas 2025 (curado y explicado)

> Para cada bloque: **Para qu√© sirve** y **C√≥mo lo usar√°s** (conectado a tu plan).

## N√∫cleo de ciencia de datos

- **NumPy / pandas** ‚Äî est√°ndar de arrays y DataFrames.  
  **Usar√°s**: EDA, *feature engineering*, *pipelines* tabulares.
- **Polars** ‚Äî DataFrames **muy r√°pidos** (n√∫cleo en Rust, *lazy*).  
  **Usar√°s**: ETL y anal√≠tica a gran escala en 1 m√°quina (suele reemplazar pandas cuando el dataset crece).
- **DuckDB** ‚Äî OLAP **in‚Äëprocess** (SQL), Parquet/CSV/S3.  
  **Usar√°s**: *joins* y agregaciones masivas desde notebooks o scripts, complementando pandas/Polars.

## ML cl√°sico

- **scikit‚Äëlearn** ‚Äî *pipelines*, m√©tricas, validaci√≥n, *model selection*.  
  **Usar√°s**: base de regresi√≥n/clasificaci√≥n/clustering + **TimeSeriesSplit**.
- **XGBoost / LightGBM / CatBoost** ‚Äî *boosting* SOTA en tabular.  
  **Usar√°s**: tabular serio (churn, fraude). CatBoost para categ√≥ricas pesadas.

## Deep Learning

- **PyTorch 2.x** ‚Äî DL flexible (eager) y ecosistema enorme.  
  **Usar√°s**: MLP/CNN/Transformers, *transfer learning* y p√©rdidas custom.  
- **TensorFlow/Keras** ‚Äî API alto nivel + **TFLite** en m√≥vil/edge.  
  **Usar√°s**: exportar a **Android** (on‚Äëdevice).  
- **JAX** (opcional) ‚Äî *jit/vmap/pmap* para investigaci√≥n num√©rica.  
- **timm / torchmetrics / Lightning** ‚Äî *model zoo*, m√©tricas, orquestaci√≥n de *training*.  
- **MLflow** ‚Äî *experiment tracking* y **model registry** (est√°ndar de facto).

## Visi√≥n por Computadora

- **OpenCV** ‚Äî preprocesado y utilidades de imagen/video.  
- **Albumentations** ‚Äî *augmentation* r√°pido y flexible.  
- **Ultralytics YOLO** ‚Äî detecci√≥n/segmentaci√≥n/pose con *training* sencillo.

## NLP / LLM

- **Hugging Face (Transformers/Datasets/Tokenizers)** ‚Äî *pipelines* y *model zoo*.  
  **Usar√°s**: *fine‚Äëtuning* ligero (LoRA), inferencia y **embeddings**.  
- **spaCy** ‚Äî NLP productivo (tokenizaci√≥n/NER).  
- **SentencePiece/Tokenizers** ‚Äî *subword* para vocabularios personalizados.
- **Servidores LLM**: **vLLM** (alto throughput), **TGI** (server listo), **llama.cpp/Ollama** (local).  
  **Usar√°s**: servir modelos en **RAG** y prototipos locales.

## RAG / B√∫squeda vectorial

- **FAISS** / **pgvector** / **Qdrant/Weaviate/Milvus** ‚Äî √≠ndices y DBs vectoriales.  
  **Usar√°s**: b√∫squeda sem√°ntica en RAG y recomendadores h√≠bridos.  
- **LangChain / LlamaIndex** ‚Äî *chains*, *retrievers*, *re‚Äëranking*.  
  **Usar√°s**: orquestar RAG con herramientas y *guardrails*.

## Evaluaci√≥n, calidad y observabilidad

- **Evidently** ‚Äî *drift/monitoring*.  
- **Great Expectations** ‚Äî **tests de datos** (calidad).  
- **Ragas / DeepEval / LM‚ÄëEval‚ÄëHarness** ‚Äî evaluaci√≥n de RAG/LLM.  
- **Langfuse** ‚Äî **trazas** y anal√≠tica para LLM apps.

## Serving e Inferencia

- **FastAPI + Pydantic v2** ‚Äî APIs r√°pidas y tipadas.  
- **ONNX Runtime / NVIDIA Triton / BentoML / Ray Serve / KServe** ‚Äî inferencia en CPU/GPU/K8s.  
  **Usar√°s**: exportar modelos a **ONNX** (portabilidad) y servir en CPU/GPU.

## Orquestaci√≥n / MLOps

- **DVC** ‚Äî versionado de datos.  
- **MLflow** ‚Äî experimentos y **model registry**.  
- **Airflow / Prefect / Dagster** ‚Äî *pipelines* y scheduling (Prefect = DX muy limpia).

## Web y M√≥vil (on‚Äëdevice)

- **Transformers.js / ONNX Runtime Web / TensorFlow.js** ‚Äî inferencia en navegador (WebGPU/WASM).  
- **TFLite/LiteRT / ML Kit / ONNX Runtime Mobile** ‚Äî inferencia en Android/iOS.  
  **Usar√°s**: PoCs *serverless* y apps m√≥viles privadas/offline.

---

# Stacks de referencia y **c√≥mo los usar√°s**

### 1) Tabular (regresi√≥n/clasificaci√≥n)
**Stack**: pandas/Polars ‚Üí scikit‚Äëlearn + (XGBoost/LightGBM) ‚Üí MLflow ‚Üí FastAPI/ONNX Runtime ‚Üí Airflow/Prefect.  
**Por qu√©**: m√°xima eficacia en datos empresariales (tabulares).  
**C√≥mo lo usar√°s**: **churn, fraude, scoring** con API `/predict`, dashboard Vue y *monitoring* con Evidently.

### 2) Forecast por SKU (Mes 6‚Äì7)
**Stack**: pandas/Polars ‚Üí statsmodels (ETS/SARIMA) + baselines ‚Üí backtesting (*walk‚Äëforward*) ‚Üí FastAPI + Vue ‚Üí MLflow/Evidently.  
**C√≥mo lo usar√°s**: **motor de compras** ‚Üí `SS/ROP/EOQ`, priorizaci√≥n **ABC/XYZ**, endpoint `/purchase_suggestions`.

### 3) NLP cl√°sico (spam/intents)
**Stack**: spaCy/NLTK ‚Üí TF‚ÄëIDF + LinearSVC/LogReg (o DistilBERT) ‚Üí FastAPI.  
**C√≥mo lo usar√°s**: filtros de **spam** y **intents** en soporte/tickets.

### 4) RAG de documentaci√≥n interna
**Stack**: ingesti√≥n/chunking ‚Üí embeddings (HF/SBERT) ‚Üí FAISS/pgvector ‚Üí LangChain/LlamaIndex ‚Üí vLLM/TGI ‚Üí Ragas/Langfuse.  
**C√≥mo lo usar√°s**: **asistente** de conocimiento con **citas** y **evaluaci√≥n** continua.

### 5) Visi√≥n (defectos/SKU)
**Stack**: OpenCV + Albumentations ‚Üí PyTorch/timm o YOLO ‚Üí ONNX/TFLite ‚Üí Triton/Android.  
**C√≥mo lo usar√°s**: control de **calidad** y conteo/clasificaci√≥n r√°pida en m√≥vil.

### 6) In‚Äëbrowser
**Stack**: Transformers.js u ONNX Runtime Web.  
**C√≥mo lo usar√°s**: demo sin backend (privacidad, cero latencia de red).

### 7) Android on‚Äëdevice
**Stack**: TFLite/ML Kit/ONNX Runtime Mobile.  
**C√≥mo lo usar√°s**: **apps offline** (visi√≥n/NLP) integradas con tu stack Kotlin.

---

# Consejos pr√°cticos (de profesional a profesional)

- **Elige DataFrame**: si haces ETL/anal√≠tica pesada en 1 m√°quina, **Polars + DuckDB** = combo r√°pido y barato.  
- **Sirve LLMs sin dolor**: empieza por **vLLM** (prod) u **Ollama** (local/dev). **TGI** si prefieres server listo.  
- **Eval√∫a RAG desde el d√≠a 1**: integra **Ragas** en CI y traza con **Langfuse**.  
- **Observabilidad**: *drift* con **Evidently**; **Great Expectations** para *data tests*; re‚Äëentrenos **gated** por m√©tricas de negocio.  
- **Exportabilidad**: compila a **ONNX** temprano ‚Üí abre **Triton**, **ORT Web/Mobile**, **KServe**.

---

# Qu√© a√±adir al calendario (puntos concretos)

- **Mes 1 ‚Äî Semana 1.5 (nuevo)**: repaso de √°lgebra/c√°lculo (arriba) + 2‚Äì3 *katas*/d√≠a.  
  **Entregables**: *cheat‚Äësheet* A4 + notebook de derivadas/PCA + mini‚Äëquiz.
- **Mes 3 ‚Äî Feature Store (1 d√≠a)**: mira **Feast** si compartir√°s *features* entre equipos.  
- **Mes 4 ‚Äî Tracking serio**: estandariza **MLflow** (experimentos + *model registry*; tags: dataset, git sha, semilla).  
- **Mes 9 ‚Äî RAG**: a√±ade **Ragas** y *prompt hardening*.  
- **Mes 12 ‚Äî Serving**: ensaya **Triton** (GPU) y **ONNX Runtime** (CPU/Edge) + *smoke tests* en CI.

---

# Mini‚Äëglosario extra

- **SVD vs Eig**: SVD funciona para cualquier matriz; Eig solo en cuadradas.  
- **Lipschitz**: cota al cambio; te gu√≠a en *step size*.  
- **MAP**: MLE con *prior* ‚Üí L2 ‚âà **prior Gaussiano**, L1 ‚âà **Laplace**.  
- **vLLM**: servidor LLM con *PagedAttention* (alto *throughput*).  
- **TGI**: servidor de generaci√≥n (Hugging Face) listo para prod.

---

# Siguientes pasos inmediatos

1. Imprime tu **cheat‚Äësheet** y crea el **notebook** de derivadas + PCA.  
2. Elige **Polars + DuckDB** o **pandas** para tu EDA base y registra todo con **MLflow**.  
3. Para **RAG**: prueba **FAISS + LangChain + vLLM** con un PDF del negocio y mide con **Ragas**.

---

## Cr√©ditos y recursos (r√°pidos)

- Libros: *Hands‚ÄëOn Machine Learning* (G√©ron), *Forecasting: Principles and Practice* (Hyndman), *Deep Learning* (Goodfellow).  
- Docs oficiales: scikit‚Äëlearn, pandas/Polars, statsmodels, PyTorch, TensorFlow, MLflow, FastAPI, Hugging Face, DuckDB.  
- Herramientas: Label Studio, DVC/MLflow, Great Expectations, Airflow/Prefect, Evidently, Langfuse.

---

> **Pro tip**: este README es tu ‚Äúcontrato‚Äù de aprendizaje. Cada semana **marca entregables**, sube *screenshots* a `reports/` y escribe un **post‚Äëmortem** corto con lo aprendido y qu√© har√°s distinto la pr√≥xima vez.
