ğŸ§® Ãlgebra lineal para IA â€” guÃ­a intensiva y prÃ¡ctica

Meta: dominar lo que realmente usarÃ¡s en IA: proyecciones, mÃ­nimos cuadrados, autovalores, SVD, PCA, 
condiciÃ³n/estabilidad y el mini-cÃ¡lculo matricial que aparece en ML.
Entregable: un notebook con PCA desde cero, y mini-proyectos de proyecciones y mÃ­nimos cuadrados.

0) Resultados de aprendizaje
Al terminar podrÃ¡s:
Construir PCA por SVD y por eig de la covarianza, explicar EVR y elegir k.
Resolver ğ´ğ‘¥â‰ˆğ‘
Axâ‰ˆb por normales, QR y SVD; saber cuÃ¡ndo usar cada una.
Detectar mal condicionamiento y aplicar soluciones estables (escalado, SVD, regularizaciÃ³n).
Usar pseudoinversa, whitening, truncado de SVD (Eckartâ€“Young).
Derivar y verificar con autograd las identidades de cÃ¡lculo matricial que verÃ¡s en ML.

1) Espacios vectoriales, bases y dimensiÃ³n
Espacio vectorial ğ‘‰ (sobre R): cerrado a suma y escalado.
Base = conjunto LI que genera V. DimensiÃ³n = tamaÃ±o de una base.
Rango rank(A) = dim. de la imagen; nulidad = dim. del nÃºcleo.
Rango-nulidad: si ğ´âˆˆğ‘…ğ‘šÃ—ğ‘›AâˆˆRmÃ—n: rank(ğ´)+nullity(ğ´)=ğ‘› rank(A)+nullity(A)=n.
En IA: pensar tus datos/embeddings como vectores en ğ‘…ğ‘‘Rd. Mucho de â€œreducir dimensiÃ³nâ€ es elegir otra base (mejor).

0) La idea general (en una frase)
Un espacio vectorial es un â€œmundoâ€ donde puedes sumar objetos (vectores) y estirarlos/encogerlos con nÃºmeros,
y esas operaciones siempre funcionan bien (sin sorpresas). 
Con eso podemos hablar de direcciones bÃ¡sicas (bases),
de cuÃ¡ntas direcciones independientes tienes (dimensiÃ³n) y de cÃ³mo describir cualquier vector usando esas direcciones.

